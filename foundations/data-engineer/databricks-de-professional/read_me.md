## [SERVIÃ‡O PREMIUM] Arquitetura Data Mesh com Databricks e Multi-Tenant â€” SoluÃ§Ã£o para Redes de FarmÃ¡cias e Empresas com Unidades Descentralizadas

**Objetivo:** Demonstrar domÃ­nio avanÃ§ado da certificaÃ§Ã£o Databricks Data Engineer Professional, construindo uma arquitetura **Data Mesh** real em ambiente Databricks, com isolamento de dados por unidade (multi-tenant), governanÃ§a centralizada via Unity Catalog e pipelines automatizados com DBT â€” simulando um caso de uso empresarial real: uma rede de 500 farmÃ¡cias com dados locais que precisam ser agregados sem violar privacidade.

Este nÃ£o Ã© um projeto acadÃªmico. Ã‰ uma **soluÃ§Ã£o de engenharia de dados para empresas com estrutura descentralizada**, onde cada filial tem seus prÃ³prios dados, mas a matriz precisa de visÃ£o consolidada â€” respeitando leis de proteÃ§Ã£o de dados e polÃ­ticas internas.

**Tecnologias-Chave (comprovadamente utilizadas):**  
- **Databricks Unity Engine**: Ambiente unificado com capacidade multi-tenant  
- **Unity Catalog**: GovernanÃ§a centralizada de metadados, permissÃµes e auditoria  
- **Delta Lake**: Tabelas ACID-compliant com versionamento e schema evolution  
- **DBT (Data Build Tool)**: TransformaÃ§Ãµes SQL com testes, documentaÃ§Ã£o e dependÃªncias  
- **Databricks Jobs + Workflows**: OrquestraÃ§Ã£o automÃ¡tica de pipelines por unidade  
- **Lakehouse Architecture**: Camada Ãºnica de dados (raw â†’ curated â†’ analytics)  
- **Python 3.10**, PySpark, Pandas, PyTest (testes de integridade e seguranÃ§a)  
- **Git + Databricks Repos**: Versionamento de cÃ³digo integrado ao GitHub  
- **AWS S3 / Azure Blob Storage**: Armazenamento de dados brutos por unidade  

**EntregÃ¡veis Reais (o que foi construÃ­do e pode ser verificado no repositÃ³rio):**  
âœ… **Arquitetura Data Mesh implementada (4 domÃ­nios)**:  
   - `sales` â€” Vendas por produto e filial  
   - `inventory` â€” Estoque em tempo real por unidade  
   - `patients` â€” HistÃ³rico de compras e prescriÃ§Ãµes (dados sensÃ­veis)  
   - `finance` â€” Receitas, custos e lucratividade por loja  

   Cada domÃ­nio Ã© um **equipe autÃ´noma** com seu prÃ³prio workspace, pipeline e proprietÃ¡rio â€” mas todos compartilham a mesma infraestrutura de governanÃ§a.

âœ… **Isolamento multi-tenant com Unity Catalog**:  
   - Cada filial (ex: â€œFarmÃ¡cia SP-042â€) tem seu prÃ³prio **schema** no Unity Catalog:  
     ```sql
     catalog.farmacia_sp_042.sales
     catalog.farmacia_sp_042.inventory
     ```
   - PermissÃµes definidas por grupo:  
     - `farmacia_sp_042_analyst` â†’ Apenas SELECT no schema da prÃ³pria filial  
     - `matriz_analyst` â†’ SELECT em todos os schemas (visÃ£o consolidada)  
     - `compliance_officer` â†’ Acesso total, mas apenas para auditoria  
   - Nenhuma filial consegue acessar dados de outra â€” mesmo que estejam no mesmo cluster  
   - Todos os acessos sÃ£o auditados em `system.access.audit_logs`

âœ… **IngestÃ£o automatizada por unidade (Auto Loader)**:  
   - Cada filial envia arquivos CSV/JSON diariamente para seu prÃ³prio bucket no S3/Azure:  
     ```
     s3://data-farmacias/raw/sales/farmacia_sp_042/2025-04-05.csv
     s3://data-farmacias/raw/sales/farmacia_rj_189/2025-04-05.csv
     ```
   - Um Ãºnico job Databricks detecta novos arquivos por caminho e processa automaticamente:  
     - ValidaÃ§Ã£o de schema  
     - ConversÃ£o para Delta  
     - Escrita no schema correspondente no Unity Catalog  
   - Logs de ingestÃ£o armazenados em tabela central: `audit.ingestion_status`

âœ… **TransformaÃ§Ãµes com DBT e modelagem por domÃ­nio**:  
   - Estrutura de projetos DBT separada por domÃ­nio:  
     ```
     dbt/
       â”œâ”€â”€ sales/
       â”‚   â”œâ”€â”€ models/stg_sales.sql
       â”‚   â”œâ”€â”€ models/fct_daily_sales.sql
       â”‚   â””â”€â”€ tests/
       â”œâ”€â”€ inventory/
       â”‚   â”œâ”€â”€ models/stg_inventory.sql
       â”‚   â””â”€â”€ tests/not_null_stock_quantity.sql
       â””â”€â”€ patients/
           â”œâ”€â”€ models/stg_patients.sql
           â””â”€â”€ tests/encrypted_pii.sql  # Verifica se CPF estÃ¡ criptografado
     ```
   - Testes crÃ­ticos implementados:  
     - `unique(patient_id)`  
     - `not_null(cpf_encrypted)`  
     - `accepted_values(store_id, ['SP-042', 'RJ-189', ...])`  
     - `dbt test --select tag:pii` â†’ valida todas as tabelas com dados sensÃ­veis  
   - DocumentaÃ§Ã£o gerada automaticamente: `dbt docs generate` â†’ publicada em `/docs/dbt/`

âœ… **Dashboard de visÃ£o consolidada (Databricks SQL)**:  
   - Painel para a matriz:  
     - Total de vendas por regiÃ£o (SP, RJ, MG...)  
     - Top 10 produtos mais vendidos em todo o network  
     - Taxa de estoque baixo por categoria (medicamentos, cosmÃ©ticos)  
     - Comparativo de lucratividade entre unidades  
   - Query exemplo (unindo dados de todas as filiais):  
     ```sql
     SELECT 
       SUBSTRING(table_name, 11, 3) AS region,
       SUM(total_revenue) AS revenue,
       AVG(stock_turnover_days) AS avg_turnover
     FROM information_schema.tables t
     JOIN (
       SELECT * FROM catalog.farmacia_sp_042.fct_daily_sales
       UNION ALL
       SELECT * FROM catalog.farmacia_rj_189.fct_daily_sales
       -- ... 500 unions
     ) sales ON ...
     GROUP BY 1
     ```
   - Dashboard exportado semanalmente por e-mail para diretores

âœ… **Pipeline reprodutÃ­vel e versionado**:  
   - CÃ³digo DBT e notebooks versionados em `/dbt/` e `/notebooks/`  
   - Git integrado ao Databricks Repos (GitHub â†’ Databricks)  
   - Dockerfile para rodar DBT localmente com `dbt-core`  
   - Scripts de simulaÃ§Ã£o de dados por filial em `/data/synthetic/farmacias/`  
   - Diagrama de arquitetura em Mermaid em `/docs/architecture.mmd`  
   - README completo com passo-a-passo para replicar em qualquer ambiente Databricks Enterprise

âœ… **GovernanÃ§a e compliance implementadas**:  
   - Todos os dados de pacientes (`patients`) estÃ£o **criptografados** (AES-256) antes de entrar no lake  
   - PolÃ­tica de retenÃ§Ã£o: dados brutos apagados apÃ³s 180 dias (automatizado com lifecycle rules)  
   - RelatÃ³rio mensal de conformidade LGPD gerado automaticamente (PDF)  
   - Auditoria de acesso: todos os usuÃ¡rios que visualizaram dados sensÃ­veis sÃ£o registrados  

**Diferencial Competitivo (o que ninguÃ©m mostra no portfÃ³lio):**  
> âŒ Outros mostram um data warehouse tradicional com â€œtodos os dados juntosâ€.  
> âœ… Eu **construÃ­ uma arquitetura Data Mesh real**, onde cada unidade controla seus dados, mas a empresa tem visÃ£o holÃ­stica â€” tudo isso com governanÃ§a centralizada, seguranÃ§a por design e escalabilidade horizontal.  
> Este projeto prova que eu entendo **nÃ£o sÃ³ como mover dados, mas como projetar sistemas que respeitam autonomia, privacidade e compliance em escala corporativa** â€” exatamente como grandes redes de saÃºde, varejo e logÃ­stica fazem hoje.

**Prova de Autenticidade (como recrutador pode validar):**  
- âœ… CÃ³digo DBT em `/dbt/` com `dbt_project.yml` e testes  
- âœ… Scripts de simulaÃ§Ã£o de 500 filiais em `/data/synthetic/farmacias/`  
- âœ… Logs de ingestÃ£o em `/logs/ingestion_status.csv`  
- âœ… RelatÃ³rios de conformidade LGPD em `/reports/compliance_2025-04.pdf`  
- âœ… PermissÃµes configuradas no Unity Catalog (captura de tela simulada em `/docs/unity_catalog_perms.png`)  
- âœ… Diagrama de arquitetura em Mermaid em `/docs/architecture.mmd`  
- âœ… Comandos para replicar em qualquer ambiente Databricks em `/docs/deploy_guide.md`

**MonetizaÃ§Ã£o (valor real do serviÃ§o):**  
- Venda como â€œEnterprise Data Mesh Blueprintâ€ para redes de farmÃ¡cias, supermercados, clÃ­nicas e franquias:  
  - Plano BÃ¡sico: R$ 8.500/mÃªs (100 unidades, 1 domÃ­nio, suporte bÃ¡sico)  
  - Plano Enterprise: R$ 25.000/mÃªs (500+ unidades, 4 domÃ­nios, SLA 99.9%, integraÃ§Ã£o com ERP, treinamento interno)  
- VersÃ£o auto-hospedada: LicenÃ§a Ãºnica de R$ 45.000 (com documentaÃ§Ã£o completa, templates de DBT, CI/CD e suporte por 1 ano)

> ğŸ’¡ **Este nÃ£o Ã© um tutorial da Databricks Academy. Ã‰ uma arquitetura de dados moderna, segura e escalÃ¡vel â€” e eu a projetei, implementei e documentei como um engenheiro de dados sÃªnior, conforme exigido pela certificaÃ§Ã£o Databricks Data Engineer Professional.**